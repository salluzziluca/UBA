El Procesamiento Natural del Lenguaje. Es una intersección entre linguistica, computacion e IA que busca que la computadora entienda el lenguaje humano.

Sirve para: 
- Entender la emocion detrás de cada pagina de texto.
- Detectar los temas en comun entre muchos textos (TT en twitter)
- Moderar foros de mensajes
- Busqueda de documentos
- traducción


## Tokenizar
hay diferentes formas de tokenizar, una muy comun es como se muestra abajo
```
"Hola, cómo estás?"->["hola", ",", "cómo", "estás", "?"]
```

Es lo primero que se hace cuando tengo que hacer NLP

## Bag of words

tomo las primeras k palabras mas repetidas de nuestra muestra
![[Pasted image 20230911103237.png]]
![[Pasted image 20230911100947.png]]
![[Pasted image 20230911103610.png]]
Luego armamos la matriz y calculamos 
el coseno entre el vector query y los diferentes textos

## Term Frequency (Count Vectorizer)

en la tabla se pone la cantidad de veces que aparece la palabra
![[Pasted image 20230911104133.png]]

![[Pasted image 20230911104210.png]]

## TF-IDF: term frequency x inverse document frequency
$$IDF(palabra)=\log\left( \frac{N+1}{frecuencia} \right)$$

siendo N: cantidad total de documentos y frecuencia: cantidad de documentos en los que aparece la palabra. Va a dar alto si la palabra no aparece en muchos docus y va a dar bajo si aparece en muchos.

![[Pasted image 20230911105028.png]]
![[Pasted image 20230911105142.png]]


## Normalizacion

hacer que diferentes palabras vayan al mismo token
ej: comer, comemos, comeré ----> comer
### stemming
vamos a la raiz de la palabra
caballo, caballeria->caball
canto, canta, cantamos, cantan ->cant
cantidad, cantidades ->cant
### Lemmatization
devuelve la base de la palabra (lemma), usualmente sacado de un diccionario hecho a mano
canto, canta, cantamos, cantan ->canto
cantidad, cantidades ->cantidad

## Stopwords
Son palabras vacias, como articulos, preposiciones

- NLP sirve para hacer procesamiento o IA sobre textos o voz
- Para dividir un texto en unidades hablamos de tokens
- Para buscar textos pasamos cada texto a un vecto siguiendo alguna regla (BOW, TF, TF-IDF), luego usamos la similaridad coseno
- Podemos normalizar los textos para que las plaabras con significado similar vayan al mismo token
- Es comun eliminar las stopwords antes de crear estos vectores o de hacer distintos analisis sobtre los textos