---
dg-publish: true
---
La comunicación se realiza entre el driver y los executors.
Las jobs del driver se convierten en tareas para los executors, estos luego devuelven los resultados al driver


## Resilent Distributed Datasets (RDDs)
-  Colecciones particionadas en un clustering. DE esta forma se subdivide la data
- guardados en memoria o disco
- Reconstruidos automaticamente freante a fallos de máquinas o demores en un job
- Creados a partir de datos externos
### transformaciones
Crean un nuevo RDD a partir de otro existente. 

![[Pasted image 20230918115232.png]]
![[Pasted image 20230918115139.png]]

### acciones
- Devuelve un valor al driver luego de procesar los datos
- ![[Pasted image 20230918115218.png]]
Para leer un archivo puedo hacer 
`rdd = spark.sparContext.textFile('s.txt')'`
o
`rdd = sc.parallelize(var, numeroDeParalelizaciones)`


## Acciones
count(x) = muestra los primeros x elementos
collect muestra todos
first muestra el primero
takeOrderer(cantidad, key=lambda). Devuelve n elementos segun el criterio que le demos
takeSample(bool, x) muestra x registros con o sin reemplazo
reduce(lambda) hace cosas, devuelve 1 solo registro
countByKey:
![[Pasted image 20230918120818.png]]

## Transformaciones 
Map(lambda): transforma los resgistros en base a la func dada
Filter(lambda) devuelve un booleano (filtra) segun si cumple o no con esa funcion
FlatMap(lambda, lambda, lambda), puede devolver mas de 1 registro
ReduceByKey(lambda) combina los registros para una misma clave en base a una funcion de reduce
GroupByKey(lambda), me va a devolver los mismos registros, pero el par va a ser $[clave, \ nuevoValor]$
Distinct elimina los duplicados


`RDD1.union(RDD2)` obtiene la union entre los dos RDD
`RDD1.intersection(RDD2)` devuelve los valores que estan en ambos RDD
`RDD1.substract(RDD2)` saca del RDD1 los valores que comparta con el RDD2
