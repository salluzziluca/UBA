La comunicación se realiza entre el driver y los executors.
Las jobs del driver se convierten en tareas para los executors, estos luego devuelven los resultados al driver


## Resilent Distributed Datasets (RDDs)
-  Colecciones particionadas en un clustering. DE esta forma se subdivide la data
- guardados en memoria o disco
- Reconstruidos automaticamente freante a fallos de máquinas o demores en un job
- Creados a partir de datos externos
### transformaciones
Crean un nuevo RDD a partir de otro existente. 

![[Pasted image 20230918115232.png]]
![[Pasted image 20230918115139.png]]

### acciones
- Devuelve un valor al driver luego de procesar los datos
- ![[Pasted image 20230918115218.png]]
Para leer un archivo puedo hacer 
`rdd = spark.sparContext.textFile('s.txt')'`
o
`rdd = sc.parallelize(var, numeroDeParalelizaciones)`


## Acciones
count(x) = muestra los primeros x elementos
collect muestra todos
first muestra el primero
takeOrderer