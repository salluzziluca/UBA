---
dg-publish: true
---
Creado por Claude Shanon

## Bits de Información
Para saber cuantos bits de informacion estamos realmente transmitiendo Claude nos da la formula 
$$bits = -\log_{2}(P)$$
Siendo P la [[1.1 Definición de Probabilidad|probabilidad]] de que ese evento ocurra

## Entropía de Shanon

$$H=P_{1}.-\log_{2}(P_{1})+\dots+P_{n}.-\log_{2}(P_{n})$$
Nos indica que tan impredecible es un fenomeno
## Entropía Cruzada
$$H(p,q)= \sum^{n}_{i=1}P_{i}.-\log_{2}(Q_{i})$$
Siendo Q mi sistema y P la realidad.
Entonces H(p, q) -H(p) es el costo extra que pago porque mi sistema no se  parece tanto a la realidad
## Divergencia de Kullback-Leiber
Es ese costo extra
![[Pasted image 20230925115955.png]]

Finalmente H(p,q)= H(p)+DK(p||q)