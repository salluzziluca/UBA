---
dg-publish: true
---
 do Entrenar un modelo es un problema de optimizacion. Buscamos los param optimos para minimizar o maximizar la metrica elegida. Una vez entranado, el modelo queda definido por sus parametros.

Los ==hiperparametros== son aquellos que parametrizan el entrenamiento de nuestro modelo
![[Pasted image 20231012162034.png]]

Hay que buscar los mejores hiper-parametros (tuning)

## Tuning
### Grid-search
>mas lenta pero uso todos los datos

Establecemos un conjunto de valores a probar por cada hiper-parámetro
probamos cada combinacion de hipers y nos quedamos con la mejor. Una vez que encontramos los valores optimos podemos refinar la busqueda (si el optimo está en un extremo, por ejemplo, o asi está entre dos valores)

### Random search
>le asignamos el tiempo que nostros disponemos

tomamos un conjunto de valores igual que en grid, probamos k combinaciones aleatorias y nos quedamos con la mejor (decidimos cuanto tiempo invertir pero no pro)

### Búsqueda Bayesiana

definimos una dist de proba y rangos por cada hiper-parametro. El algoritmo va probando combinaciones de hiper parametros de forma bayesiana
![[Pasted image 20231012163221.png]]

## Kfold cross valitadion
Como no podemos usar el set de test para entrenar al modelo, tenemos que partir al set de entrenamiento en 2: Train y Validation. Para esto partimos el set de entrenamiento en k particiones, tomando 1 fold como validacion y el resto como train, luego vamos rotando
![[Pasted image 20231012164010.png]]

## Underfitting
Se produce cuando tenemos un error de entrenamiento alto, el modelo se ajusta mal al set. Suele pasar porque el modelo no tiene suficiente capacidad expresiva o no tiene complejidad o grados de libertad

## Overfitting 
El modelo tiene muy bien ressultado para el set de entrenamiento pero no lo es para el set de test. Generaliza mal, alucina, es demasiado expresivo
Para solucionar el overfitting podemos conseguir mas datos, lo volvemos mas complejo o lo regularizamos
![[Pasted image 20231012170159.png]] underfitting izquierda, correcto centro, overfitting derecha


### Regularizacion
Es una tecnica que penaliza la complejidad de un modelo

por ejemplo haciendo lambda: $x*y$ siendo los grados de complejidad de los polinomios
![[Pasted image 20231012170704.png]]