## KNN
K-nearest neighbors 
Hay que buscar los k puntos mas cercanos a un cierto punto dado.

Si k es muy chico: [[7.3 Evaluacion de Modelos#Overfitting|overfitting]]
si k es muy grande: [[7.3 Evaluacion de Modelos#Underfitting|underfitting]] (porque tomaria 1 elemento con TODOS vecinos)
![[Pasted image 20231012171629.png]]

si m vecinos de un punto son de otra clase -> el punto es un outlier

Para buscar un k ideal pruebo mucho y los voy comparando 
![[Pasted image 20231012172407.png]]

Es necesario determinar k y la distancia a usar, no siempre la euclideana es la mejor. KNN es muy sensible a features ruidosas (ej: correlacion altura-edad)

1. Calculo de la distencia de la nueva observacion a cada uno de los puntos que se tienen en el set de entrenamiento (obtengo un vector de distancias)
2. ordeno ese vector de menor a mayor
3. selecciono los primeros k puntos de ese vector
4. utilizar esos k puntos para predecir 
	1. [[7.2 Métricas y Errores#Clasificación binaria|clasificación]]->se toman los k vecinos mas cercanos y se clasficican por voto de la mayoria
	2. [[7.2 Métricas y Errores#Regresión|regresión]]->se toman los k vecinos mas cercanos y se toma el promedio

### Teorema de Cover-Hart
Si el mejor clasificador posible tiene un error e entonces KNN con datos infinitos tiene un error 2e


## Regresion lineal 
Quremos expresar una variable Y en funcion de otras p variables X1, X2, ..., Xp
$$y_{i}= \beta_{0}+\beta_{1}x_{i_{1}}+\dots+\beta_{n}.x_{ip}+\epsilon_{i}$$
En un ejemplo la glucosa en sangre de una persona (y_i) es igual a la cantidad de azucar que comio (x_1) por un coeficiente (B_1) + la cantidad de ejercicio que hizo por otro coeficiente y asi. Siendo $\epsilon_{i}$ el error aleatorio
![[Pasted image 20231016201001.png]]

## Regresión logistica
se una para casos de [[7.2 Métricas y Errores#Clasificación binaria|clasificacion]]
utilizar una regresion 