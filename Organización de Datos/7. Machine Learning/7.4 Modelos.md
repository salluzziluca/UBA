## KNN
K-nearest neighbors 
Hay que buscar los k puntos mas cercanos a un cierto punto dado.

Si k es muy chico: [[7.3 Evaluacion de Modelos#Overfitting|overfitting]]
si k es muy grande: [[7.3 Evaluacion de Modelos#Underfitting|underfitting]] (porque tomaria 1 elemento con TODOS vecinos)
![[Pasted image 20231012171629.png]]

si m vecinos de un punto son de otra clase -> el punto es un outlier

Para buscar un k ideal pruebo mucho y los voy comparando 
![[Pasted image 20231012172407.png]]

Es necesario determinar k y la distancia a usar, no siempre la euclideana es la mejor. KNN es muy sensible a features ruidosas (ej: correlacion altura-edad)

### Teorema de Cover-Hart
Si el mejor clasificador posible tiene un error e entonces KNN con datos infinitos tiene un error 2e


## Regresion lineal 
Quremos expresar una variable Y en funcion de otras p variables X1, X2, ..., Xp
$$y_{i}= \beta_{0}+\beta_{1}x_{i_{1}}+\dots+\beta_{n}.x_{ip}+\epsilon_{i}$$
En un ejemplo la glucosa en sangre de una persona (y_i) es igual a la cantidad de azucar que comio (x_1) por un coeficiente (B_1) + la cantidad de ejercicio que hizo por otro coeficiente y asi. Siendo $\epsilon_{i}$ el error aleatorio
![[Pasted image 20231016201001.png]]