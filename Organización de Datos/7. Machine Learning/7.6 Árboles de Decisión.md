Son arboles con ramas paras cada valor de la comparación.
En cada nodo dividimos el set de datos de acuerdo a cierto criterio.

Nuestro objetivo es llegar a nodos hoja en los cuales podamos clasificar correctamente nuestros datos

### Ventajas
- Simples de entender y de interpretar (caja blanca)
- Funcionan con datos numéricos o categóricos
-  Requiere poca preparación de los datos: No hay que normalizar
- Buena performance para datasets grandes (un feature que aparece mas arriba seguramente sea más importante)
- Ayudan a la Selección de features
### Limitaciones
- Encontrar el óptimo es NP-complete
	- Algoritmos greedy para encontrar óptimos locales
-  Árboles demasiado complejos generan overfitting

## Algoritmos
### ID3
Algoritmo Greedy:
- en cada paso se realiza el mejor split posible en dos
- Selecciona el atributo que nos da mayor Ganancia de Información(relacionados con la entropía)
Esto se repite recursivamente, los features deben ser categóricos.

![[Pasted image 20231023115018.png]]

### C4.5
Es el sucesor de ID3, acepta atributos numéricos y datos con atributos faltantes. Los atributos pueden tener un peso


## Ensambles
Es cuando combinamos distintos arboles de decision para tener un mejor clasificador. Los mejores algoritmos de ML son combinacion de varios algoritmos.

#### Bagging
1. Aplicar el mismo clasificador n veces usando bootstrapping: 
> tomamos muestras del set de entrenamiento (con reemplazo) de igual tamaño que este

2. Promediar sus resultados
![[Pasted image 20231023120523.png]]
Mediante 3 clasificadores lineales muy simples logré clasificar todo el set correctamente.

##### ventajas
disminuye el overfitting
Puedo utilizar los registros Out of Bag para ver la precision del algoritmo.

#### Boosting
1. Entrena un algoritmo simple
2. analiza suis resultados
3. Entrena otro algoritmo simple en donde se le da mayor peso a los resultados para los cuales el anterior tuvo peor performance
4. Resultado final en base a un promedio ponderado 


#### Majority Voting
- Cada clasificador da un voto a cada clase
- Mejor si se usan resultados con poca correlación
- Se le puede dar un peso distinto a cada modelo
![[Pasted image 20231023122132.png]]

#### Averaging
- Promediamos el resultado de varios clasificadores
- Sirve para regresión y clasificación (con clases o probas)
- Ajuste de escala de probas de cada modelo usando rango entre 1 y n (total de registros). Prob $=1-\frac{x-min}{max-min}$

#### Blending o Stacked
- Separar la parte pequeña del set de entrenamiento (10%)
-  Con los clasificadores entrenados coon el resto del set, lo usamos para predecir sobre el set anterior.
- Con esto se entrena un nuevo modelo para que aprenda cómo combinar los resultados de los clasificadores de forma tal que estos nos den la predicción final

## Random Forest
Algoritmo de [[#Bagging]] para arboles de decisión