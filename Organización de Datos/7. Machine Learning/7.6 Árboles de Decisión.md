---
dg-publish: true
---
Son [[Arboles]] con ramas paras cada valor de la comparación.
En cada nodo dividimos el set de datos de acuerdo a cierto criterio.

Nuestro objetivo es llegar a nodos hoja en los cuales podamos clasificar correctamente nuestros datos

### Ventajas
- Simples de entender y de interpretar (caja blanca)
- Funcionan con datos numéricos o categóricos
-  Requiere poca preparación de los datos: No hay que normalizar
- Buena performance para datasets grandes (un feature que aparece mas arriba seguramente sea más importante)
- Ayudan a la Selección de features
### Limitaciones
- Encontrar el óptimo es NP-complete
	- [[Algoritmos Greedy]] para encontrar óptimos locales
-  Árboles demasiado complejos generan overfitting

## Algoritmos
### ID3
[[Algoritmos Greedy|Algoritmo Greedy]]:
- en cada paso se realiza el mejor split posible en dos
- Selecciona el atributo que nos da mayor Ganancia de Información(relacionados con la entropía)
Esto se repite recursivamente, los features deben ser categóricos.

![[Pasted image 20231023115018.png]]

### C4.5
Es el sucesor de ID3, acepta atributos numéricos y datos con atributos faltantes. Los atributos pueden tener un peso


## Ensambles
Es cuando combinamos distintos arboles de decision para tener un mejor clasificador. Los mejores algoritmos de ML son combinacion de varios algoritmos.

#### Bagging
1. Aplicar el mismo clasificador n veces usando bootstrapping: 
> tomamos muestras del set de entrenamiento (con reemplazo) de igual tamaño que este

2. Promediar sus resultados
![[Pasted image 20231023120523.png]]
Mediante 3 clasificadores lineales muy simples logré clasificar todo el set correctamente.

##### ventajas
disminuye el overfitting
Puedo utilizar los registros Out of Bag para ver la precision del algoritmo.

#### Boosting
1. Entrena un algoritmo simple
2. analiza suis resultados
3. Entrena otro algoritmo simple en donde se le da mayor peso a los resultados para los cuales el anterior tuvo peor performance
4. Resultado final en base a un promedio ponderado 


#### Majority Voting
- Cada clasificador da un voto a cada clase
- Mejor si se usan resultados con poca correlación
- Se le puede dar un peso distinto a cada modelo
![[Pasted image 20231023122132.png]]

#### Averaging
- Promediamos el resultado de varios clasificadores
- Sirve para regresión y clasificación (con clases o probas)
- Ajuste de escala de probas de cada modelo usando rango entre 1 y n (total de registros). Prob $=1-\frac{x-min}{max-min}$

#### Blending o Stacked
- Separar la parte pequeña del set de entrenamiento (10%)
-  Con los clasificadores entrenados coon el resto del set, lo usamos para predecir sobre el set anterior.
- Con esto se entrena un nuevo modelo para que aprenda cómo combinar los resultados de los clasificadores de forma tal que estos nos den la predicción final

## Random Forest
Algoritmo de [[#Bagging]] para arboles de decisión. Usa un subconjunto de los atributos y un bootstrap del set de entrenamiento. 
Se construyen n arboles con conjuntos diferentes de datos (mediante bootstrapping) y atributos (tomados al azar). Luego los vamos a combinar para dar con el resultado final.

Hiper-parametros:
- Cantidad de [[Arboles]] a crear (más [[Arboles]] mejores resultados pero trade-off performance)
- Cantidad de atributos por arbol (más critico)

![[Pasted image 20231023123441.png]]

## XG Boost 
Bosteo de [[Arboles]] de decisión

$$Obj(x)=L(x)+ \Omega(x)$$
con X: parámetros a aprender
L: error del modelo
Omega: factor de regularizacion
$$L(x)=\sum^{n}_{i=1}l(y_{i}, \hat{y}_{i})$$
l es el error en la prediccion de Y
Para regresión puede ser -> $l = (y_i -hat{y}_i)^2$
Para clasficiación binaria -> $$l=y_{i}\ln(1+e^{-\hat{y}_{i}})+(1-y_{i})\ln(1+e^{\hat{y}_{i}})$$
La prediccion es la sumatoria de la prediccion de varios arboles

![[Pasted image 20231023124432.png]]

Complejidad:
![[Pasted image 20231023124549.png]]
![[Pasted image 20231023124708.png]]
![[Pasted image 20231023124720.png]]