Son arboles con ramas paras cada valor de la comparación.
En cada nodo dividimos el set de datos de acuerdo a cierto criterio.

Nuestro objetivo es llegar a nodos hoja en los cuales podamos clasificar correctamente nuestros datos

### Ventajas
- Simples de entender y de interpretar (caja blanca)
- Funcionan con datos numéricos o categóricos
-  Requiere poca preparación de los datos: No hay que normalizar
- Buena performance para datasets grandes (un feature que aparece mas arriba seguramente sea más importante)
- Ayudan a la Selección de features
### Limitaciones
- Encontrar el óptimo es NP-complete
	- Algoritmos greedy para encontrar óptimos locales
-  Árboles demasiado complejos generan overfitting

## Algoritmos
### ID3
Algoritmo Greedy:
- en cada paso se realiza el mejor split posible en dos
- Selecciona el atributo que nos da mayor Ganancia de Información(relacionados con la entropía)
Esto se repite recursivamente, los features deben ser categóricos.

Hay un gran peligro de overfitting

