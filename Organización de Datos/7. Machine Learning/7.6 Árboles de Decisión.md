Son arboles con ramas paras cada valor de la comparación.
En cada nodo dividimos el set de datos de acuerdo a cierto criterio.

Nuestro objetivo es llegar a nodos hoja en los cuales podamos clasificar correctamente nuestros datos

### Ventajas
- Simples de entender y de interpretar (caja blanca)
- Funcionan con datos numéricos o categóricos
-  Requiere poca preparación de los datos: No hay que normalizar
- Buena performance para datasets grandes (un feature que aparece mas arriba seguramente sea más importante)
- Ayudan a la Selección de features
### Limitaciones
- Encontrar el óptimo es NP-complete
	- Algoritmos greedy para encontrar óptimos locales
-  Árboles demasiado complejos generan overfitting

## Algoritmos
### ID3
Algoritmo Greedy:
- en cada paso se realiza el mejor split posible en dos
- Selecciona el atributo que nos da mayor Ganancia de Información(relacionados con la entropía)
Esto se repite recursivamente, los features deben ser categóricos.

![[Pasted image 20231023115018.png]]

### C4.5
Es el sucesor de ID3, acepta atributos numéricos y datos con atributos faltantes. Los atributos pueden tener un peso


## Ensambles
Es cuando combinamos distintos arboles de decision para tener un mejor clasificador. Los mejores algoritmos de ML son combinacion de varios algoritmos.

#### Bagging
1. Aplicar el mismo clasificador n veces usando bootstrapping: 
> tomamos muestras del set de entrenamiento (con reemplazo) de igual tamaño que este

2. Promediar sus resultados
![[Pasted image 20231023120523.png]]
Mediante 3 clasificadores lineales muy simples logré clasificar todo el set correctamente.

##### ventajas
disminuye el overfitting
Puedo utilizar los registros Out of Bag para ver la precision del algoritmo.

#### Boosting
1. Entrena un algoritmo simple
2. analiza suis resultados
3. Entre otro algoritmo simple en donde se le da mayor peso a los resultados para los cuales el anterior tuvo peor performance
4. Resultado final en base a un promedio ponderado 