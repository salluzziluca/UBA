---
dg-publish: true
---
	Metodo para construir estimadores puntuales. Se basan que en los experimentos aleatorios los resultados deben alta proba de ocurrir 

>  Diremos que $\hat{\theta}(\underline{X} )$ es un estimador de máximo Verosimilitud de $\theta$ si se cumple que $$f_{\hat{\theta}}(\underline{X})=\max_{\theta} f_{\theta}(\underline{x})$$
>  El valor que maximice la proba conjunta evaluada en la muestra observada. Es decir, que maximice la [[9.1 Introducción a la Estadística#Funcion de Verosimilitud|funcion de verosimilitud]]

Entonces: Busco el theta que maximice L(theta) y despues calculo $\hat{\Theta}=\hat{\Theta}(\underline{X})$

![[Pasted image 20231109172447.png]]


# Principio de invarianza. 
si $\lambda=q(\theta)$ es una funcion biunivoca de $\theta$. Y $\hat{\theta}$ es el Estimador de Maxima Verosimilitud de $\theta$, entonces $\hat{\lambda}=q(\hat{\theta})$será el EMV de $\lambda$


# Bondad de los Estimadores
Dada $X_{1}, \dots, X_{n} \sim^{idd} F_{\theta}(x), \ \theta \in \Theta$ una muestra aleatoria. Estimamos $\theta$ por $\hat{\theta}$. El riesgo de la estimacion se mide como el![[3.4 Predicciones#error cuadratico medio]] con parametros $\theta$, $\hat{\theta}$
Un estiamador optimo será $\hat{\Theta}^*$ tal que $ECM(\hat{\Theta}^*)\leq ECM(\hat{\Theta})$

Se dice que un estimador es ==insesgado==  si $E_{\theta}(\hat{\theta})=\theta$. En caso contrario, el ==sesgo== se define como $B(\hat{\theta})= E_{\theta}(\hat{\theta})-\theta$ 

$$ECM(\hat{\theta})=var(\hat{\theta})+B(\hat{\theta})^2 $$


# Consistencia
> Dada una sucesión de estimadores $\hat{\theta}_{n} \ de \ \theta$, decimos que $T=\hat{\theta}$ es debilmente consistente si $\forall \varepsilon > 0$, $$P_{\theta}(|T-\theta|>\varepsilon)\to_{n\to\infty} 0$$

Sea una sucesion de estimadores $\hat{\theta}_{n}$ de $\theta$. Si $var_\theta(\hat{\theta})\to{0}$ y $E_{\theta}(\hat{\theta})\to 0$ entonces $\hat{\theta}_{n}$ es debilmente consitente

# Estimadores asintóticamente normales

Se dice que $\hat{\theta}_{n}$ es una sucesion de estimadores asintóticamente normales si  $\sqrt{ n }(\hat{\theta}_{n}-q(\theta))$converge en distribución a una normal con media 0 y varianza $\frac{q'(\theta)^2}{I(\theta)}$
$I(\theta)$ es el ==numero de información de fisher== y se calcula como 

$$I(\theta)=E\left[ \left( \frac{d}{d \theta} \ln(f_{\theta}(X))^2\right) \right]=-E\left[ \frac{d²}{d \theta^2} \ln(f_{\theta}(X))\right]$$

Teorema: Bajo ciertas condicionmes generales, sea $\hat{\theta}_{n}(\underline{X})$ un EMV de $\theta$ ==consistente== y sea $q(\theta)$ derivable con $q'(\theta)\neq 0, \ \forall \theta$, entonces $q(\hat{\theta}_{n})$ es asintóticamente normal para estimar a $q(\theta)$.

Si $\sqrt{ n }\sqrt{ I(\theta) }(\hat{\theta}_{n}-\theta)\sim^{(a)}N(0,1)$ y $\hat{\theta}_{n}$ es un estimador consistente para $\theta$ (todo esto ocurrirá con los EMV), entonces vale que $$\sqrt{ n }\sqrt{ I(\hat{\theta}_{n}) }(\hat{\theta}_{n}-\theta) \sim^{a} Normal(0,1)$$


$$\\begin{equation}

\end{ecuation}L(\beta_{0}, \beta_{1})=\prod^{n}_{i=1} \frac{1}{\sqrt{ 2 \pi } \sigma}e^{\frac{1}{2 \sigma^2}(y_{i}+\beta_{0}+\beta_{1}x_{i})^2}
\end{equation}$$
$$\ln(L(\beta_{0}, \beta_{1}))=\frac{n}{\sqrt{ 2 \pi  }\sigma}+ \sum^{n}_{i=1}\frac {1}{2 \sigma^2}(-y_{i}+\beta_{0}+\beta_{1}x_{i})^2
$$
## I)
$$\frac{\partial \ln(L(\beta_{0}, \beta_{1}))}{\partial \beta_{0}}=\frac{1}{2 \sigma^2 }\sum^{n}_{i=1}(-y_{i}+\beta_{0}+\beta_{1}x_{i})=0$$
$$\sum^{n}_{i=1}(-y_{i}+\beta_{0}+\beta_{1}x_{i})=0$$
$$\hat{\beta}_{0}=\frac{1}{n }\sum^{n}_{i=1}(y_{i}-\hat{\beta}_{1}x_{i})$$

## II)
$$\frac{\partial \ln(L(\beta_{0}, \beta_{1}))}{\partial \beta_{1}}=\frac{1}{2 \sigma^2 }\sum^{n}_{i=1}(-y_{i}+\beta_{0}+\beta_{1}x_{i})x_{i}=0$$
utilizando I)
$$
\sum^{n}_{i=1}\left[ y_{i-\frac{1}{n}}\sum^{n}_{i=1}((y_{i}-\hat{\beta}_{1}x_{i})-\beta_{1}x_{i})x_{i} \right]=0
$$

$$
\hat{\beta}_{1}=\frac{\sum^{n}_{i=1}(x_{i}-\bar{x}).(y_{i}-\bar{y})}{\sum^{n}_{i=1}(x_{i}-\bar{x})^2}
$$