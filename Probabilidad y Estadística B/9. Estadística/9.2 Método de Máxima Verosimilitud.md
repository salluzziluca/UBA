Metodo para construir estimadores puntuales. Se basan que en los experimentos aleatorios los resultados deben alta proba de ocurrir 

>  Diremos que $\hat{\theta}(\underline{X} )$ es un estimador de máximo Verosimilitud de $\theta$ si se cumple que $$f_{\hat{\theta}}(\underline{X})=\max_{\theta} f_{\theta}(\underline{x})$$
>  El valor que maximice la proba conjunta evaluada en la muestra observada. Es decir, que maximice la [[9.1 Introducción a la Estadística#Funcion de Verosimilitud|funcion de verosimilitud]]

Entonces: Busco el theta que maximice L(theta) y despues calculo $\hat{\Theta}=\hat{\Theta}(\underline{X})$

![[Pasted image 20231109172447.png]]


# Principio de invarianza. 
si $\lambda=q(\theta)$ es una funcion biunivoca de $\theta$. Y $\hat{\theta}$ es el Estimador de Maxima Verosimilitud de $\theta$, entonces $\hat{\lambda}=q(\hat{\theta})$será el EMV de $\lambda$


# Bondad de los Estimadores
Dada $X_{1}, \dots, X_{n} \sim^{idd} F_{\theta}(x), \ \theta \in \Theta$ una muestra aleatoria. Estimamos $\theta$ por $\hat{\theta}$. El riesgo de la estimacion se mide como el![[3.4 Predicciones#error cuadratico medio]] con parametros $\theta$, $\hat{\theta}$
Un estiamador optimo será $\hat{\Theta}^*$ tal que $ECM(\hat{\Theta}^*)\leq ECM(\hat{\Theta})$

Se dice que un estimador es ==insesgado==  si $E_{\theta}(\hat{\theta})=\theta$. En caso contrario, el sesgo se define como $B(\hat{\theta})= E_{\theta}(\hat{\theta})-\theta$ 

$$ECM(\hat{\theta})=var(\hat{\theta})+B(\hat{\theta})^2 $$


# Consistencia
> Dada una sucesión de estimadores $\hat{\theta}_{n} \ de \ \theta$, decimos que $T=\hat{\theta}$ es debilmente consistente si $\forall \varepsilon > 0$, $$P_{\theta}(|T-\theta|>\varepsilon)\to_{n\to\infty} 0$$

Sea una sucesion de estimadores $\hat{\theta}_{n}$ de $\theta$. Si $var_\theta(\hat{\theta})\to{0}$ y $E_{\theta}(\hat{\theta})\to 0$ entonces $\hat{\theta}_{n}$ es debilmente consitente

# Estimadores asintóticamente normales

Se dice que $\hat{\theta}_{n}$ es una sucesion de estimadores asintóticamente normales si  $\sqrt{ n }(\hat{\theta}_{n}-q(\theta))$converge en distribución a una normal con media 0 y varianza $\frac{q'(\theta)^2}{I(\theta)}$
$I(\theta)$ es el ==numero de información de fisher== y se calcula como 

$$I(\theta)=E\left[ \left( \frac{d}{d \theta} \ln(f_{\theta}(X))^2\right) \right]=-E\left[ \frac{d²}{d \theta^2} \ln(f_{\theta}(X))\right]$$

Teorema: Bajo ciertas condicionmes generales, sea $\hat{\theta}_{n}(\underline{X})$ un EMV de $\theta$ ==consistente== y sea $q(\theta)$ derivable con $q'(\theta)\neq 0, \ \forall \theta$, entonces $q(\hat{\theta}_{n})$ es asintóticamente normal para estimar a $q(\theta)$.

Si $\sqrt{ n }\sqrt{ I(\theta) }(\hat{\theta}_{n}-\theta)\sim^{(a)}N(0,1)$ y $\hat{\theta}_{n}$ es un estimador consistente para $\theta$ (todo esto ocurrirá con los EMV), entonces vale que $$\sqrt{ n }\sqrt{ I }$$